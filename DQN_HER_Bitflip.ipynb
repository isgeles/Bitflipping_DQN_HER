{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitflipping with DQN and Hindsight Experience Replay (HER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import pandas as pd                # rolling average plot\n",
    "import progressbar as pb           # tracking time while training\n",
    "\n",
    "from Bitflipping_Environment import BitFlippingEnv\n",
    "from dqn_agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bitflipping(n, n_episodes=5, policy=None, render=False):\n",
    "    env = BitFlippingEnv(n)\n",
    "    success = []\n",
    "\n",
    "    for e in range(n_episodes):\n",
    "        state, _, _, _ = env.reset()\n",
    "        if render:\n",
    "            env.render()\n",
    "        for t in range(n):\n",
    "            \n",
    "            if policy is None:\n",
    "                action = np.random.randint(0, n)\n",
    "            else:\n",
    "                state_goal = np.concatenate([state['obs'], state['goal']])\n",
    "                action = policy.act(state_goal, eps=0)\n",
    "            \n",
    "            state, reward, done, info = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "        success.append(int(info))\n",
    "    return np.mean(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# testing random actions in env\n",
    "test_bitflipping(n=5, n_episodes=3, render=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PARAMS = {\n",
    "    'n_bits': 30,                 # n bits to flip in environment (n corresponding target bits)\n",
    "    'seed': 0,                    # random seed for environment, torch, numpy, random packages\n",
    "\n",
    "    'eps': 0.2,             # probability of random action, 'epsilon-greedy' policy\n",
    "    \n",
    "    'buffer_size': int(1e6),      # replay-buffer size\n",
    "    'batch_size': 128,             # mini-batch size\n",
    "    'gamma': 0.98,                # discount factor\n",
    "    'tau': 0.05,                  # soft update of target network, 1-tau = polyak coefficient\n",
    "    'lr': 0.001,                  # learning rate\n",
    "\n",
    "    # training setup\n",
    "    'replay_strategy': 'final',    # 'final' or 'future' replay strategy for HER\n",
    "    'n_epochs': 200,              # number of epochs, HER paper: 200 epochs (i.e. maximum of 8e6 timesteps)\n",
    "    'n_cycles': 50,               # number of cycles per epoch, HER paper: 50 cycles\n",
    "    'n_episodes': 16,             # number of episodes per cycle, HER paper: 16 episodes\n",
    "    'n_optim': 10,                 # number of optimization steps every cycle, HER paper: 40 steps (only necessary for higher number of bits)\n",
    "}\n",
    "\n",
    "\n",
    "def set_seeds(seed: int = 0):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    pass\n",
    "\n",
    "set_seeds(DEFAULT_PARAMS['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n, agent):\n",
    "    print(\"Training DQN on Bitflipping with\", n, \"bits for\", DEFAULT_PARAMS['n_epochs'], \"epochs...\")\n",
    "\n",
    "    # widget bar to display progress during training\n",
    "    widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA()]\n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=DEFAULT_PARAMS['n_epochs']).start()\n",
    "\n",
    "    env = BitFlippingEnv(n)\n",
    "    success = []\n",
    "    eps = DEFAULT_PARAMS['eps']\n",
    "    for i_epoch in range(1, DEFAULT_PARAMS['n_epochs'] + 1):\n",
    "        for i_cycle in range(DEFAULT_PARAMS['n_cycles']):\n",
    "            for i_episode in range(DEFAULT_PARAMS['n_episodes']):\n",
    "                state, _, _, _ = env.reset()\n",
    "                state_ep, act_ep, reward_ep, next_state_ep, done_ep = [], [], [], [], []\n",
    "                for t in range(1000):\n",
    "                    state_goal = np.concatenate([state['obs'], state['goal']])\n",
    "                    action = agent.act(state_goal, eps)\n",
    "                    next_state, reward, done, info = env.step(action)\n",
    "\n",
    "                    # save current transition of episode\n",
    "                    state_ep.append(state.copy())\n",
    "                    act_ep.append(action)\n",
    "                    reward_ep.append(reward)\n",
    "                    next_state_ep.append(next_state.copy())\n",
    "                    done_ep.append(done)\n",
    "\n",
    "                    state = next_state\n",
    "                    if done:\n",
    "                        break\n",
    "                success.append(int(info))\n",
    "                # for standard experience replay\n",
    "                agent.store_episode(state_ep, act_ep, reward_ep, next_state_ep, done_ep)  \n",
    "                # HER: save additional goals\n",
    "                agent.store_episode_HER(state_ep, act_ep, reward_ep, next_state_ep, done_ep,\n",
    "                                        replay_strategy=DEFAULT_PARAMS['replay_strategy'])\n",
    "            # optimize and soft update of networks\n",
    "            for _ in range(DEFAULT_PARAMS['n_optim']):\n",
    "                agent.learn()\n",
    "            agent.soft_update(agent.qnetwork_local, agent.qnetwork_target, agent.tau)\n",
    "\n",
    "        # stop training earlier\n",
    "        if np.mean(success[-50:]) > 0.95:\n",
    "            print(\"\\n learning done\")\n",
    "            break\n",
    "\n",
    "        if i_epoch % (DEFAULT_PARAMS['n_cycles'] / 10) == 0:\n",
    "            print('\\rEpoch {} \\t Success: {:.4f}'.format(i_epoch, np.mean(success[-10:])))\n",
    "        timer.update(i_epoch)\n",
    "    timer.finish()\n",
    "    return success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "agent = Agent(DEFAULT_PARAMS['n_bits'], DEFAULT_PARAMS['n_bits'],\n",
    "              DEFAULT_PARAMS['batch_size'], DEFAULT_PARAMS['buffer_size'], DEFAULT_PARAMS['gamma'],\n",
    "              DEFAULT_PARAMS['tau'], DEFAULT_PARAMS['lr'])\n",
    "\n",
    "success = train(DEFAULT_PARAMS['n_bits'], agent)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Results"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rolling average of success\n",
    "N = 300\n",
    "rolling_avg = pd.Series(success).rolling(window = N).mean().iloc[N-1:].values\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(rolling_avg)), rolling_avg)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving trained network\n",
    "torch.save(agent.qnetwork_local.state_dict(), './trained/checkpoint_'+str(DEFAULT_PARAMS['n_bits'])+'bits.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 30\n",
    "env = BitFlippingEnv(n)\n",
    "agent = Agent(state_size=n, action_size=n, seed=0)\n",
    "\n",
    "# load the weights from file\n",
    "# agent.qnetwork_local.load_state_dict(torch.load('./trained/checkpoint_30bits.pth'))\n",
    "\n",
    "test_bitflipping(n=n, n_episodes=3, policy=agent, render=True) "
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "print(\"Testing agent for 100 episodes, success-rate: \",\n",
    "          test_bitflipping(DEFAULT_PARAMS['n_bits'], n_episodes=100, policy=agent)*100, \"%\")"
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
