{
 "cells": [
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "# Bitflipping with DQN and Hindsight Experience Replay (HER)"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Packages"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 1,
   "metadata": {},
   "outputs": [],
   "source": [
    "import numpy as np\n",
    "import torch\n",
    "import random\n",
    "import matplotlib.pyplot as plt\n",
    "import progressbar as pb           # tracking time while training\n",
    "\n",
    "from Bitflipping_Environment import BitFlippingEnv\n",
    "from dqn_agent import Agent"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Testing Environment"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 2,
   "metadata": {},
   "outputs": [],
   "source": [
    "def test_bitflipping(n, n_episodes=5, policy=None, render=False):\n",
    "    env = BitFlippingEnv(n)\n",
    "    success = []\n",
    "\n",
    "    for e in range(n_episodes):\n",
    "        state, _, _, _ = env.reset()\n",
    "        if render:\n",
    "            env.render()\n",
    "        for t in range(n):\n",
    "            \n",
    "            if policy is None:\n",
    "                action = np.random.randint(0, n)\n",
    "            else:\n",
    "                state_goal = np.concatenate([state['obs'], state['goal']])\n",
    "                action = policy.act(state_goal, eps=0)\n",
    "            \n",
    "            state, reward, done, info = env.step(action)\n",
    "            if render:\n",
    "                env.render()\n",
    "            if done:\n",
    "                break\n",
    "        success.append(int(info))\n",
    "    return np.mean(success)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 3,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Step  0    Bits: [0 0 0 1 0]   Goal: [1 1 0 1 0]   Success: False\n",
      "Step  1    Bits: [0 0 1 1 0]   Goal: [1 1 0 1 0]   Success: False\n",
      "Step  2    Bits: [0 0 1 0 0]   Goal: [1 1 0 1 0]   Success: False\n",
      "Step  3    Bits: [1 0 1 0 0]   Goal: [1 1 0 1 0]   Success: False\n",
      "Step  4    Bits: [1 0 0 0 0]   Goal: [1 1 0 1 0]   Success: False\n",
      "Step  5    Bits: [1 0 0 1 0]   Goal: [1 1 0 1 0]   Success: False\n",
      "DONE in 5 timesteps, success: False \n",
      "\n",
      "Step  0    Bits: [0 0 0 0 1]   Goal: [1 0 1 1 0]   Success: False\n",
      "Step  1    Bits: [0 0 1 0 1]   Goal: [1 0 1 1 0]   Success: False\n",
      "Step  2    Bits: [0 1 1 0 1]   Goal: [1 0 1 1 0]   Success: False\n",
      "Step  3    Bits: [1 1 1 0 1]   Goal: [1 0 1 1 0]   Success: False\n",
      "Step  4    Bits: [1 1 0 0 1]   Goal: [1 0 1 1 0]   Success: False\n",
      "Step  5    Bits: [1 0 0 0 1]   Goal: [1 0 1 1 0]   Success: False\n",
      "DONE in 5 timesteps, success: False \n",
      "\n",
      "Step  0    Bits: [0 0 1 0 0]   Goal: [0 1 1 1 0]   Success: False\n",
      "Step  1    Bits: [0 1 1 0 0]   Goal: [0 1 1 1 0]   Success: False\n",
      "Step  2    Bits: [0 1 1 1 0]   Goal: [0 1 1 1 0]   Success: True\n",
      "DONE in 2 timesteps, success: True \n",
      "\n"
     ]
    },
    {
     "data": {
      "text/plain": [
       "0.3333333333333333"
      ]
     },
     "execution_count": 3,
     "metadata": {},
     "output_type": "execute_result"
    }
   ],
   "source": [
    "# testing random actions in env\n",
    "test_bitflipping(n=5, n_episodes=3, render=True)  "
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Training Setup"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 4,
   "metadata": {},
   "outputs": [],
   "source": [
    "DEFAULT_PARAMS = {\n",
    "    'n_bits': 20,                 # n bits to flip in environment (n corresponding target bits)\n",
    "    'seed': 0,                    # random seed for environment, torch, numpy, random packages\n",
    "\n",
    "    'eps': 0.2,             # probability of random action, 'epsilon-greedy' policy\n",
    "    \n",
    "    'buffer_size': int(1e6),      # replay-buffer size\n",
    "    'batch_size': 128,             # mini-batch size\n",
    "    'gamma': 0.98,                # discount factor\n",
    "    # TODO\n",
    "    'tau': 0.05,                  # soft update of target network, 1-tau = polyak coefficient\n",
    "    'lr': 0.001,                  # learning rate\n",
    "\n",
    "    # training setup\n",
    "    'replay_strategy': 'future',    # 'final' or 'future' replay strategy for HER\n",
    "    'n_epochs': 200,              # number of epochs, HER paper: 200 epochs (i.e. maximum of 8e6 timesteps)\n",
    "    'n_cycles': 50,               # number of cycles per epoch, HER paper: 50 cycles\n",
    "    'n_episodes': 16,             # number of episodes per cycle, HER paper: 16 episodes\n",
    "    'n_optim': 40,                # number of optimization steps every cycle, HER paper: 40 steps\n",
    "}\n",
    "\n",
    "\n",
    "def set_seeds(seed: int = 0):\n",
    "    torch.backends.cudnn.deterministic = True\n",
    "    torch.backends.cudnn.benchmark = False\n",
    "    torch.manual_seed(seed)\n",
    "    torch.cuda.manual_seed_all(seed)\n",
    "    np.random.seed(seed)\n",
    "    random.seed(seed)\n",
    "    pass\n",
    "\n",
    "set_seeds(DEFAULT_PARAMS['seed'])"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": 5,
   "metadata": {},
   "outputs": [],
   "source": [
    "def train(n, agent):\n",
    "    print(\"Training DQN on Bitflipping with\", n, \"bits for\", DEFAULT_PARAMS['n_epochs'], \"epochs...\")\n",
    "\n",
    "    # widget bar to display progress during training\n",
    "    widget = ['training loop: ', pb.Percentage(), ' ', pb.Bar(), ' ', pb.ETA()]\n",
    "    timer = pb.ProgressBar(widgets=widget, maxval=DEFAULT_PARAMS['n_epochs']).start()\n",
    "\n",
    "    env = BitFlippingEnv(n)\n",
    "    success = []\n",
    "    eps = DEFAULT_PARAMS['eps']\n",
    "    for i_epoch in range(1, DEFAULT_PARAMS['n_epochs'] + 1):\n",
    "        for i_cycle in range(DEFAULT_PARAMS['n_cycles']):\n",
    "            for i_episode in range(DEFAULT_PARAMS['n_episodes']):\n",
    "                state, _, _, _ = env.reset()\n",
    "                state_ep, act_ep, reward_ep, next_state_ep, done_ep = [], [], [], [], []\n",
    "                for t in range(DEFAULT_PARAMS['n_bits']):\n",
    "                    state_goal = np.concatenate([state['obs'], state['goal']])\n",
    "                    action = agent.act(state_goal, eps)\n",
    "                    next_state, reward, done, info = env.step(action)\n",
    "\n",
    "                    state_ep.append(state.copy())\n",
    "                    act_ep.append(action)\n",
    "                    reward_ep.append(reward)\n",
    "                    next_state_ep.append(next_state.copy())\n",
    "                    done_ep.append(done)\n",
    "                    \n",
    "                    success.append(int(info))\n",
    "                    if done:\n",
    "                        break\n",
    "                    state = next_state\n",
    "                agent.store_episode(state_ep, act_ep, reward_ep, next_state_ep, done_ep)\n",
    "                \n",
    "                # HER additional goals\n",
    "                agent.store_episode_HER(state_ep, act_ep, reward_ep, next_state_ep, done_ep, replay_strategy=DEFAULT_PARAMS['replay_strategy'])\n",
    "               \n",
    "            for _ in range(DEFAULT_PARAMS['n_optim']):\n",
    "                agent.learn()       \n",
    "        agent.soft_update(agent.qnetwork_local, agent.qnetwork_target, agent.tau)\n",
    "        \n",
    "        # stop training\n",
    "        if np.mean(success[-10:]) > 0.999:\n",
    "            print(\"\\n learning done\")\n",
    "            break\n",
    "            \n",
    "        if i_epoch % (DEFAULT_PARAMS['n_cycles'] / 10) == 0:\n",
    "            print('\\rEpoch {} \\t Success: {:.4f}'.format(i_epoch, np.mean(success[-10:])))\n",
    "\n",
    "        timer.update(i_epoch)\n",
    "    timer.finish()\n",
    "    return success"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Start training DQN Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Training DQN on Bitflipping with 20 bits for 200 epochs...\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   2% |                                          | ETA:   1:28:59"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 5 \t Success: 0.0625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   4% |#                                         | ETA:   1:28:05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 10 \t Success: 0.5437\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   7% |##                                        | ETA:   1:27:16"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 15 \t Success: 0.8625\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:   9% |###                                       | ETA:   1:27:33"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 20 \t Success: 0.8875\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  12% |#####                                     | ETA:   1:26:05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 25 \t Success: 0.8938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  14% |######                                    | ETA:   1:27:05"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 30 \t Success: 0.8938\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  17% |#######                                   | ETA:   1:29:01"
     ]
    },
    {
     "name": "stdout",
     "output_type": "stream",
     "text": [
      "Epoch 35 \t Success: 0.9313\n"
     ]
    },
    {
     "name": "stderr",
     "output_type": "stream",
     "text": [
      "training loop:  19% |########                                  | ETA:   1:29:55"
     ]
    }
   ],
   "source": [
    "agent = Agent(DEFAULT_PARAMS['n_bits'], DEFAULT_PARAMS['n_bits'],\n",
    "              DEFAULT_PARAMS['batch_size'], DEFAULT_PARAMS['buffer_size'], DEFAULT_PARAMS['gamma'],\n",
    "              DEFAULT_PARAMS['tau'], DEFAULT_PARAMS['lr'])\n",
    "\n",
    "success = train(DEFAULT_PARAMS['n_bits'], agent)"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# Plot rolling average of success\n",
    "import pandas as pd\n",
    "\n",
    "N = 300\n",
    "rolling_avg = pd.Series(success).rolling(window = N).mean().iloc[N-1:].values\n",
    "\n",
    "fig = plt.figure()\n",
    "ax = fig.add_subplot(111)\n",
    "plt.plot(np.arange(len(rolling_avg)), rolling_avg)\n",
    "plt.ylabel('Score')\n",
    "plt.xlabel('Episode #')\n",
    "plt.show()"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "# saving trained network\n",
    "torch.save(agent.qnetwork_local.state_dict(), './trained/checkpoint_'+str(DEFAULT_PARAMS['n_bits'])+'bits.pth')"
   ]
  },
  {
   "cell_type": "markdown",
   "metadata": {},
   "source": [
    "### Test trained Agent"
   ]
  },
  {
   "cell_type": "code",
   "execution_count": null,
   "metadata": {},
   "outputs": [],
   "source": [
    "n = 45\n",
    "env = BitFlippingEnv(n)\n",
    "agent = Agent(state_size=n, action_size=n, seed=0)\n",
    "\n",
    "# load the weights from file\n",
    "# agent.qnetwork_local.load_state_dict(torch.load('./trained/checkpoint_30bits.pth'))\n",
    "\n",
    "test_bitflipping(n=5, n_episodes=3, policy=agent, render=True) "
   ]
  }
 ],
 "metadata": {
  "kernelspec": {
   "display_name": "Python 3",
   "language": "python",
   "name": "python3"
  },
  "language_info": {
   "codemirror_mode": {
    "name": "ipython",
    "version": 3
   },
   "file_extension": ".py",
   "mimetype": "text/x-python",
   "name": "python",
   "nbconvert_exporter": "python",
   "pygments_lexer": "ipython3",
   "version": "3.7.6"
  }
 },
 "nbformat": 4,
 "nbformat_minor": 4
}
